è¿™æ˜¯ä¸€ä¸ªforkè‡ª https://github.com/karpathy/nanoGPT çš„é¡¹ç›®ï¼Œæˆ‘æƒ³åŸºäºè¿™ä¸ªé¡¹ç›®æ¥å­¦ä¹ è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚

æˆ‘çš„è®¾å¤‡ä¿¡æ¯ï¼š

- ä¸€å° HP ZBook G11 ç¬”è®°æœ¬ç”µè„‘ï¼›

- CPUæ¶æ„ï¼šx86_64çš„Intel(R) Core(TM) Ultra 7 155H (1.40 GHz)ï¼›
- å†…å­˜ï¼š64Gï¼›
- æ˜¾å¡ï¼šRTX4060ï¼ˆ8Gæ˜¾å­˜ï¼‰ï¼›
- æ“ä½œç³»ç»Ÿï¼šWindows11ï¼Œå®‰è£…äº† Nvidia æ˜¾å¡ 581.29ç‰ˆæœ¬é©±åŠ¨ï¼›
- é€šè¿‡wsl2éƒ¨ç½²äº†Ubuntu24.04ï¼›

ç¡¬ä»¶é™åˆ¶ï¼š

- ä½ å¿…é¡»æ³¨æ„çš„æ˜¯ï¼Œæˆ‘åªæœ‰1å°ç¬”è®°æœ¬ç”µè„‘ï¼Œç»å¯¹ä¸è¦å°è¯•ä½¿ç”¨ä»€ä¹ˆåˆ†å¸ƒå¼è®­ç»ƒçš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¤šå°æœºå™¨ã€å¤šå¡è®­ç»ƒç­‰ã€‚

è½¯ä»¶ä¿¡æ¯ï¼š

- wls2 çš„ Ubuntu ä¸­çš„ç”¨æˆ·åä¸º kevinï¼Œhomeç›®å½•ä¸ºï¼š/home/kevinï¼›

- å®‰è£…äº† /home/kevin/miniconda3ï¼Œåˆ›å»ºäº† nanogpt è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä¸”å®‰è£…äº†éœ€è¦çš„ä¾èµ–åŒ…ï¼›

- é¡¹ç›®ç›®å½•ï¼š/home/kevin/nanoGPTï¼›

- æ•°æ®è·¯å¾„ï¼š/home/kevin/trainDataï¼Œå­˜æ”¾äº†ä» https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2 ä¸‹è½½çš„ç»´åŸºç™¾ç§‘ä¸­æ–‡è¯­æ–™ï¼ˆä½¿ç”¨å‘½ä»¤ `wikiextractor -o wiki_text zhwiki-latest-pages-articles.xml` æå–äº† zhwiki-latest-pages-articles.xmlï¼Œå­˜æ”¾åˆ° /home/kevin/trainData/wiki_text ç›®å½•ä¸­ï¼‰ï¼Œå…¶å†…å­˜æ”¾äº†1497915ç¯‡æ–‡ç« ã€‚ä½¿ç”¨å¦‚ä¸‹merge_wiki_doc.py è„šæœ¬å°†å…¶æ•°æ®æå–åˆ° /home/kevin/trainData/wiki_corpus/train.txt æ–‡ä»¶ä¸­ï¼š

  ```python
  # merge_wiki_doc.py
  import os
  from pathlib import Path
  
  def merge_wiki_doc_to_txt(input_dir, output_file):
      input_path = Path(input_dir)
      output_path = Path(output_file)
      output_path.parent.mkdir(parents=True, exist_ok=True)
  
      article_count = 0
      with open(output_file, "w", encoding="utf-8") as out_f:
          for subdir in input_path.iterdir():
              if not subdir.is_dir():
                  continue
              for file_path in sorted(subdir.iterdir()):
                  if not file_path.name.startswith("wiki_"):
                      continue
                  with open(file_path, "r", encoding="utf-8") as in_f:
                      lines = in_f.readlines()
  
                  i = 0
                  while i < len(lines):
                      line = lines[i].strip()
                      if line.startswith("<doc "):
                          i += 2  # è·³è¿‡ <doc> è¡Œ å’Œ æ ‡é¢˜è¡Œï¼ˆå¦‚â€œæ•°å­¦â€ï¼‰
                          content_lines = []
                          while i < len(lines) and not lines[i].strip().startswith("</doc>"):
                              raw_line = lines[i]
                              stripped = raw_line.rstrip('\n\r')  # ä¿ç•™å·¦ä¾§ç©ºæ ¼ï¼ˆå¦‚æœ‰ï¼‰ï¼Œä½†å»æ‰æ¢è¡Œç¬¦
                              # ä»…å½“è¯¥è¡Œæœ‰éç©ºç™½å­—ç¬¦æ—¶æ‰ä¿ç•™
                              if stripped.strip():  # å¦‚æœå»é™¤å‰åç©ºç™½åä¸ä¸ºç©º
                                  content_lines.append(stripped)
                              # å¦‚æœæ˜¯ç©ºè¡Œï¼Œç›´æ¥è·³è¿‡ï¼ˆä¸ appendï¼‰
                              i += 1
                          # è·³è¿‡ </doc>
                          i += 1
  
                          # åˆå¹¶æœ‰æ•ˆå†…å®¹ï¼ˆç”¨å•ä¸ªæ¢è¡Œè¿æ¥ï¼Œæ–‡ç« ä¹‹é—´ç”¨åŒæ¢è¡Œåˆ†éš”ï¼‰
                          if content_lines:
                              content = "\n".join(content_lines).strip()
                              if content:
                                  out_f.write(content + "\n\n")
                                  article_count += 1
                                  if article_count % 100000 == 0:
                                      print(f"âœ… å·²åˆå¹¶ {article_count} ç¯‡æ–‡ç« ")
                      else:
                          i += 1
  
      print(f"ğŸ‰ åˆå¹¶å®Œæˆï¼å…± {article_count} ç¯‡æ–‡ç« ï¼Œä¿å­˜è‡³: {output_file}")
  
  if __name__ == "__main__":
      merge_wiki_doc_to_txt(
          input_dir="/home/kevin/trainData/wiki_text",
          output_file="/home/kevin/trainData/wiki_corpus/train.txt"
      )
  ```

- llama.cppï¼šä½ç½®åœ¨ /home/kevin/llama.cppï¼Œå¹¶é€šè¿‡ `make clean && LLAMA_CUDA=1 make` å®Œæˆäº†ç¼–è¯‘ã€‚

ä½ çš„ç›®æ ‡ï¼š

1. é¦–å…ˆç”¨ nanoGPT è‡ªå¸¦çš„â€œèå£«æ¯”äºšâ€æ•°æ®é›†ï¼Œå®Œæˆæ¨¡å‹è®­ç»ƒï¼Œå¹¶è½¬æ¢æˆHuggingFaceæ ¼å¼ï¼ˆGGUFï¼‰ä½¿ç”¨ llama.cpp è£…è½½æ¨¡å‹å¹¶å®Œæˆé—®ç­”æµ‹è¯•ï¼›
2. ç„¶åå°†è¿™ä¸ªæ¨¡å‹è£…è½½åˆ° ollama ä¸­å¹¶å®Œæˆé—®ç­”æµ‹è¯•ï¼›
3. å®Œæˆè®­ç»ƒè¿‡ç¨‹ä¸­è¯¦ç»†çš„æŒ‡å¯¼æ–‡æ¡£ï¼›
4. æœ€åå†å¼€å§‹ä½¿ç”¨ /home/kevin/trainData ä¸‹çš„æ•°æ®é›†è®­ç»ƒä¸­æ–‡è¯­è¨€æ¨¡å‹ã€‚

é™åˆ¶æ¡çº¿ï¼š

- æ‰€æœ‰çš„è®­ç»ƒï¼ŒåŒ…æ‹¬å°è§„æ¨¡æ•°æ®è®­ç»ƒæµ‹è¯•æ¨¡å‹ï¼Œéƒ½å¿…é¡»ä½¿ç”¨cudaæ¥åŠ é€Ÿè®­ç»ƒï¼›
- ä½ éœ€è¦ä½¿ç”¨ `conda activate nanogpt` æ¥æ¿€æ´»è¿™ä¸ªè™šæ‹Ÿç¯å¢ƒï¼›
- åœ¨è®­ç»ƒä¸­æ–‡æ¨¡å‹æ—¶å¿…é¡»æ³¨æ„åˆ†è¯é—®é¢˜ï¼Œå¿…é¡»å’Œ ollamaã€llama.cpp åŠ è½½æ¨¡å‹å…¼å®¹åˆ†è¯å™¨ã€‚